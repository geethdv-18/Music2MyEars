# MUSIC2MYEARS â€” HACKATHON BLUEPRINT v3

## 1. Architecture Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     STREAMLIT  UI  (app.py)                         â”‚
â”‚                                                                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ INPUT SECTION â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                                                                â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”‚   â”‚
â”‚  â”‚  â”‚  Image    â”‚   â”‚  Text    â”‚   â”‚  Voice   â”‚  â† any combo     â”‚   â”‚
â”‚  â”‚  â”‚  Upload   â”‚   â”‚  Input   â”‚   â”‚  Upload  â”‚                   â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚   â”‚
â”‚  â”‚                                                                â”‚   â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ADVANCED OPTIONS (collapsed) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤   â”‚
â”‚  â”‚  â–¶ Advanced Options                                            â”‚   â”‚
â”‚  â”‚  â”Œâ”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”  â”‚   â”‚
â”‚  â”‚  â”‚  Energy:  â—‹â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—â”€â”€â”€â”€â”€â”€â”€â—‹  [__]                     â”‚  â”‚   â”‚
â”‚  â”‚  â”‚           0         50     100                           â”‚  â”‚   â”‚
â”‚  â”‚  â”‚                                                          â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  Style:   â—‹â”€â”€â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—‹  [__]   Lo-fi â†” Cinematicâ”‚  â”‚   â”‚
â”‚  â”‚  â”‚                                                          â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  Warmth:  â—‹â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—â”€â”€â”€â—‹  [__]   Warm â†” Bright    â”‚  â”‚   â”‚
â”‚  â”‚  â”‚                                                          â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  Arc:     â—‹â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—  [__]   Steady â†” Big Buildâ”‚  â”‚   â”‚
â”‚  â”‚  â””â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”˜  â”‚   â”‚
â”‚  â”‚                                                                â”‚   â”‚
â”‚  â”‚                   [ Generate Music ]                           â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                             â”‚                                        â”‚
â”‚                             â”‚  ONE click triggers entire pipeline    â”‚
â”‚                             â–¼                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ RESULTS SECTION (appears after generation) â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                                                                â”‚   â”‚
â”‚  â”‚  "We detected: melancholy"                                     â”‚   â”‚
â”‚  â”‚  AI read:  Energy 72 â”‚ Style 40 â”‚ Warmth 65 â”‚ Arc 55          â”‚   â”‚
â”‚  â”‚  Final:    Energy 72 â”‚ Style 15 â”‚ Warmth 65 â”‚ Arc 88          â”‚   â”‚
â”‚  â”‚           (highlights overrides)                                â”‚   â”‚
â”‚  â”‚                                                                â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚  â”‚  Audio Player (variation 1)    Audio Player (variation 2)â”‚  â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚  â”‚                                                                â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚  â”‚  Explanation: "Your rainy window photo and the word       â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  'longing' created a bittersweet lo-fi piece. You shifted â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  the arc toward Big Build, so it swells to a climax..."   â”‚  â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚  â”‚                                                                â”‚   â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚   â”‚
â”‚  â”‚  â”‚  Emotion Timeline (Plotly bar chart)                      â”‚  â”‚   â”‚
â”‚  â”‚  â”‚  â–ˆâ–ˆâ–ˆâ–ˆ intro:calm  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ build:hope  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ peak:joy    â”‚  â”‚   â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â”‚
â”‚  â”‚                                                                â”‚   â”‚
â”‚  â”‚  â­ Rating (1-5)    ğŸ” Would replay?    [ Submit Feedback ]   â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Backend Pipeline

```
[ Generate Music ] clicked
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      INPUT  ANALYZERS                                â”‚
â”‚                                                                      â”‚
â”‚  image_analyzer.py   text_analyzer.py   voice_analyzer.py            â”‚
â”‚  (Claude Vision)     (Claude)           (Whisper + Claude)           â”‚
â”‚       â”‚                   â”‚                   â”‚                      â”‚
â”‚       â–¼                   â–¼                   â–¼                      â”‚
â”‚  {caption, mood,    {summary, mood,     {transcript, mood,           â”‚
â”‚   energy, source}    energy, source}     energy, source}             â”‚
â”‚                                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                           â”‚  list[dict]  (mood signals)
                           â–¼
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚   EMOTION  FUSER     â”‚  â† Claude API
                 â”‚   emotion_fuser.py   â”‚
                 â”‚                      â”‚
                 â”‚  merge mood signals  â”‚
                 â”‚  into ai_profile     â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â”‚  ai_profile dict
                            â–¼
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚  SLIDER OVERRIDE     â”‚  â† app.py logic (no module)
                 â”‚                      â”‚
                 â”‚  if Advanced Options â”‚
                 â”‚  was opened and user â”‚
                 â”‚  moved sliders:      â”‚
                 â”‚    merge overrides   â”‚
                 â”‚  else:               â”‚
                 â”‚    final = ai_profileâ”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â”‚  final_profile dict
                            â–¼
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚  MUSIC ORCHESTRATOR  â”‚  â† Claude API
                 â”‚ music_orchestrator.pyâ”‚
                 â”‚                      â”‚
                 â”‚  final_profile â†’     â”‚
                 â”‚  MusicGen prompt     â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â”‚  prompt string
                            â–¼
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚   MUSIC GENERATOR   â”‚  â† HF Inference API
                 â”‚  music_generator.py  â”‚     (remote, not local)
                 â”‚                      â”‚
                 â”‚  POST prompt to HF   â”‚
                 â”‚  â†’ receive audio     â”‚
                 â”‚  â†’ return wav bytes  â”‚
                 â”‚  + 1 variation       â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â”‚  list[wav bytes]
                            â–¼
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚     EXPLAINER        â”‚  â† Claude API
                 â”‚   explainer.py       â”‚
                 â”‚                      â”‚
                 â”‚  profiles â†’ text +   â”‚
                 â”‚  timeline data       â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
                      Results Section
                  (audio, explanation,
                   timeline, feedback)
```

---

## 2. Emotional Profile Schema

### ai_profile (output of Emotion Fuser)

```
{
  "emotion":  str,     # dominant emotion label, e.g. "melancholy", "joy"
  "energy":   int,     # 0â€“100, overall intensity
  "style":    int,     # 0â€“100, 0=lo-fi intimate / 100=cinematic epic
  "warmth":   int,     # 0â€“100, 0=warm analog / 100=bright digital
  "arc":      int,     # 0â€“100, 0=steady constant / 100=big dramatic build
  "sources":  list     # ["text", "image"] â€” which inputs contributed
}
```

### final_profile (after slider override)

```
{
  "emotion":     str,     # unchanged from ai_profile (not slider-controlled)
  "energy":      int,     # slider value (or ai_profile default)
  "style":       int,     # slider value (or ai_profile default)
  "warmth":      int,     # slider value (or ai_profile default)
  "arc":         int,     # slider value (or ai_profile default)
  "sources":     list,    # unchanged from ai_profile
  "overrides":   list     # e.g. ["style", "arc"] â€” which sliders user moved
}
```

The `overrides` list tracks which sliders the user intentionally changed. This is passed to the Explainer so it can say things like *"You shifted the style toward lo-fi, so we swapped orchestral strings for a Rhodes piano."*

---

## 3. Module Breakdown

| # | Module | File | Responsibility | Input | Output |
|---|--------|------|---------------|-------|--------|
| **A** | **UI / App Shell** | `app.py` | Single-screen Streamlit page: input uploaders, collapsible Advanced Options expander with 4 sliders, Generate button, results section (audio player, explanation, timeline, feedback). On Generate: runs full pipeline, applies slider overrides if any, displays results. | User interactions | Raw inputs to analyzers; final_profile to orchestrator |
| **B** | **Image Analyzer** | `modules/image_analyzer.py` | Caption + mood + energy from image via Claude Vision | `bytes` (image) | `{caption, mood, energy, source: "image"}` |
| **C** | **Text Analyzer** | `modules/text_analyzer.py` | Summary + mood + energy from text via Claude | `str` | `{summary, mood, energy, source: "text"}` |
| **D** | **Voice Analyzer** | `modules/voice_analyzer.py` | Transcribe with Whisper, then tone/mood via Claude | `bytes` (audio) | `{transcript, mood, energy, source: "voice"}` |
| **E** | **Emotion Fuser** | `modules/emotion_fuser.py` | Merge N mood dicts into one ai_profile with all 4 slider dimensions (energy, style, warmth, arc) plus emotion label | `list[dict]` | `ai_profile dict` (see schema above) |
| **F** | **Music Orchestrator** | `modules/music_orchestrator.py` | Convert final_profile (with slider values) into a vivid MusicGen prompt. Maps numeric slider values to musical parameters (see mapping table). | `final_profile dict` | `str` (music prompt) |
| **G** | **Music Generator** | `modules/music_generator.py` | Send prompt to HF Inference API, receive audio bytes, return wav. Produces original + 1 variation (second call with tweaked prompt). No local model. | `str` (prompt) | `list[bytes]` (wav buffers) |
| **H** | **Explainer** | `modules/explainer.py` | Generate "why this music" explanation referencing both AI-detected emotion and any user overrides. Produce timeline data for visualization. | `ai_profile, final_profile, music_prompt` | `{explanation: str, timeline: list}` |
| **I** | **Feedback Store** | `modules/feedback.py` | Save rating, replay intent, ai_profile, final_profile, overrides to JSON | `feedback dict` | appended to `data/feedback.json` |
| **-** | **Claude Client** | `utils/claude_client.py` | Shared Anthropic client initialization from .env | - | `anthropic.Anthropic` instance |

---

## 4. Module G â€” Music Generator (HF Inference API)

### What changed from v2

Local MusicGen is gone. Module G now makes HTTP POST requests to the
Hugging Face Inference API. No torch, no transformers, no GPU required.

### How it works

```
generate_music(prompt, duration_sec=10)
â”‚
â”œâ”€ Load HF_TOKEN and HF_MODEL_ID from environment
â”‚
â”œâ”€ POST to https://api-inference.huggingface.co/models/{HF_MODEL_ID}
â”‚   Headers: Authorization: Bearer {HF_TOKEN}
â”‚   Body:    {"inputs": prompt}
â”‚
â”œâ”€ Response = raw audio bytes (HF returns audio/flac or audio/wav)
â”‚
â”œâ”€ For variation: tweak the prompt slightly
â”‚   (e.g. append "with subtle variation in rhythm")
â”‚   POST again with tweaked prompt
â”‚
â”œâ”€ Return list of 2 audio byte buffers
â”‚
â””â”€ If HF returns 503 (model loading): retry after wait
   If HF returns error: raise with clear message
```

### Environment variables consumed

```
HF_TOKEN       â€” Hugging Face API token (required)
HF_MODEL_ID    â€” model to call (default: facebook/musicgen-small)
```

### Error handling

- **503 Model Loading**: HF cold-starts models. Retry up to 3 times with
  increasing wait (5s, 15s, 30s). Show "Model is warming up..." in UI.
- **401 Unauthorized**: Bad token. Raise immediately with helpful message.
- **Rate limit**: Surface to user. Suggest waiting.

---

## 5. Environment & Dependencies

### .env.example

```
ANTHROPIC_API_KEY=your-anthropic-key-here
HF_TOKEN=your-huggingface-token-here
HF_MODEL_ID=facebook/musicgen-small
```

### requirements.txt

```
streamlit
anthropic
requests
python-dotenv
Pillow
plotly
openai-whisper
```

### What was removed (vs v2)

| Removed | Why |
|---------|-----|
| `torch` | No local model inference |
| `transformers` | No local model loading |
| `torchaudio` | No local audio processing |
| `scipy` | Was only needed for local wav writing |
| `soundfile` | Was only needed for local wav handling |

### What was added

| Added | Why |
|-------|-----|
| `requests` | HTTP calls to HF Inference API |

### What stays

| Package | Used by |
|---------|---------|
| `streamlit` | UI (Module A) |
| `anthropic` | Claude API calls (Modules B, C, D, E, F, H) |
| `python-dotenv` | Load .env (utils/claude_client.py, Module G) |
| `Pillow` | Image handling (Module B) |
| `plotly` | Emotion timeline chart (Module H / app.py) |
| `openai-whisper` | Voice transcription (Module D) â€” still runs locally |

**Note**: `openai-whisper` will pull in `torch` as its own dependency.
This is fine â€” torch is needed for Whisper, just not for music generation.
The key win is we no longer download or run MusicGen locally.

---

## 6. UI Responsibilities â€” Single-Screen Flow (Module A)

### Screen Layout (top to bottom, one page)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Title: Music2MyEars                    â”‚
â”‚  Subtitle: Turn expression into music   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  IMAGE UPLOAD  â”‚  TEXT INPUT  â”‚  VOICE  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â–¶ Advanced Options  (collapsed)        â”‚  â† st.expander, closed by default
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚    â”‚ Sliders appear here only when   â”‚  â”‚
â”‚    â”‚ the expander is opened. Values  â”‚  â”‚
â”‚    â”‚ start at midpoint defaults      â”‚  â”‚
â”‚    â”‚ until pipeline runs.            â”‚  â”‚
â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚           [ Generate Music ]            â”‚  â† single button
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  (Results section â€” hidden until        â”‚
â”‚   generation completes)                 â”‚
â”‚                                         â”‚
â”‚  Detected emotion + AI vs Final profile â”‚
â”‚  Audio players                          â”‚
â”‚  Explanation                            â”‚
â”‚  Emotion timeline chart                 â”‚
â”‚  Feedback form                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Interaction Flow (what happens on Generate click)

```
User clicks [ Generate Music ]
â”‚
â”œâ”€ Validate: at least one input provided
â”‚  (if not â†’ show error, stop)
â”‚
â”œâ”€ Step A: ANALYZE
â”‚  â”œâ”€ Run active analyzers
â”‚  â”‚   image_analyzer(img)  â†’  mood_dict
â”‚  â”‚   text_analyzer(txt)   â†’  mood_dict
â”‚  â”‚   voice_analyzer(aud)  â†’  mood_dict
â”‚  â”‚
â”‚  â””â”€ Run emotion_fuser(mood_list) â†’ ai_profile
â”‚
â”œâ”€ Step B: APPLY OVERRIDES
â”‚  â”œâ”€ Read current slider values from Advanced Options expander
â”‚  â”œâ”€ For each of the 4 dimensions (energy, style, warmth, arc):
â”‚  â”‚     if user_opened_advanced AND slider_value != default:
â”‚  â”‚       â†’ final_profile[dim] = slider_value
â”‚  â”‚       â†’ add dim to overrides list
â”‚  â”‚     else:
â”‚  â”‚       â†’ final_profile[dim] = ai_profile[dim]
â”‚  â”œâ”€ If Advanced Options was never opened:
â”‚  â”‚     â†’ final_profile = ai_profile (copy)
â”‚  â”‚     â†’ overrides = []
â”‚  â””â”€ final_profile["overrides"] = overrides
â”‚
â”œâ”€ Step C: GENERATE
â”‚  â”œâ”€ music_orchestrator(final_profile)  â†’ prompt string
â”‚  â”œâ”€ music_generator(prompt)            â†’ list[wav bytes]  (HF API)
â”‚  â””â”€ explainer(ai_profile, final_profile, prompt) â†’ explanation + timeline
â”‚
â””â”€ Step D: DISPLAY RESULTS
   â”œâ”€ Show: "We detected: {emotion}"
   â”œâ”€ Show: AI profile vs Final profile (highlight overrides)
   â”œâ”€ Show: audio players for each variation
   â”œâ”€ Show: explanation text in callout
   â”œâ”€ Show: Plotly emotion timeline chart
   â””â”€ Show: feedback form (rating + replay + submit)
```

### Slider Behavior Inside the Expander

```
BEFORE first generation:
  - Sliders exist inside st.expander("Advanced Options")
  - Initialized to neutral midpoints (50, 50, 50, 50)
  - Label reads: "Defaults will be set by AI after analysis"

DURING generation (Step B):
  - ai_profile is computed from fuser
  - For each slider dimension:
      if slider == 50 (untouched midpoint default):
        â†’ use ai_profile value (user didn't express preference)
      if slider != 50:
        â†’ user moved it â†’ treat as override
  - Build final_profile accordingly

AFTER generation (on re-run):
  - Sliders update to show ai_profile values from PREVIOUS run
  - Stored in st.session_state so sliders reflect actual AI output
  - User can now make informed adjustments for next generation
```

### Override Detection Logic

```
FIRST run (no prior ai_profile exists):
  slider_defaults = {energy: 50, style: 50, warmth: 50, arc: 50}
  user_interacted = slider_value != 50

SUBSEQUENT runs (ai_profile from last run in session_state):
  slider_defaults = ai_profile from previous run
  user_interacted = slider_value != ai_profile[dimension]

In both cases:
  if user never opened the expander:
    â†’ overrides = []
    â†’ final_profile = ai_profile

  if user opened expander but moved nothing:
    â†’ overrides = []
    â†’ final_profile = ai_profile

  if user opened expander and moved slider(s):
    â†’ overrides = [list of moved dimension names]
    â†’ final_profile merges ai_profile with slider values
```

---

## 7. Slider â†’ Music Parameter Mapping

The orchestrator uses this mapping to translate slider integers into musical language for the MusicGen prompt.

### Energy (0â€“100)

| Range | Tempo Feel | Dynamics | Prompt Words |
|-------|-----------|----------|-------------|
| 0â€“20 | ~60 BPM | very soft, sparse | "quiet, minimal, ambient, whisper-soft" |
| 21â€“40 | ~80 BPM | soft, gentle | "gentle, relaxed, easy-going, mellow" |
| 41â€“60 | ~100 BPM | moderate | "moderate energy, steady, flowing" |
| 61â€“80 | ~120 BPM | strong, driving | "energetic, driving, powerful, upbeat" |
| 81â€“100 | ~140 BPM | intense, maximal | "intense, explosive, soaring, maximum energy" |

### Style: Lo-fi (0) â†” Cinematic (100)

| Range | Instruments | Production | Prompt Words |
|-------|------------|-----------|-------------|
| 0â€“20 | Rhodes, tape piano, vinyl | lo-fi, warm hiss, mono | "lo-fi hip-hop, vinyl crackle, tape warble, bedroom" |
| 21â€“40 | acoustic guitar, soft keys | indie, understated | "indie, acoustic, intimate, small-room" |
| 41â€“60 | piano, light strings, pads | balanced, polished | "polished, balanced, clean production" |
| 61â€“80 | full strings, brass hints | cinematic, wide stereo | "cinematic, orchestral swell, wide stereo, dramatic" |
| 81â€“100 | full orchestra, choir, percussion | epic, Hans Zimmer | "epic orchestral, massive, soaring strings, choir, blockbuster" |

### Warmth: Warm (0) â†” Bright (100)

| Range | Tone | Textures | Prompt Words |
|-------|------|---------|-------------|
| 0â€“20 | dark, round, analog | tape saturation, sub bass | "warm analog, dark tone, round bass, vintage" |
| 21â€“40 | warm, smooth | soft reverb, muted highs | "warm, smooth, soft, muted, cozy" |
| 41â€“60 | neutral, balanced | natural room tone | "natural, balanced tone, clear" |
| 61â€“80 | bright, clear | crisp highs, shimmer | "bright, crisp, shimmering, airy, sparkling" |
| 81â€“100 | very bright, digital | synthetic sparkle, glass | "crystalline, digital, glass-like, ultra-bright, neon" |

### Arc: Steady (0) â†” Big Build (100)

| Range | Structure | Dynamics Over Time | Prompt Words |
|-------|----------|-------------------|-------------|
| 0â€“20 | single section, no change | flat dynamics | "steady, unchanging, ambient loop, constant" |
| 21â€“40 | subtle swell | gentle variation | "gentle variation, subtle breathing, light swell" |
| 41â€“60 | verse-chorus feel | moderate dynamics | "evolving, verse-chorus, moderate build" |
| 61â€“80 | clear build + climax | rising to peak | "building, rising intensity, crescendo, climax" |
| 81â€“100 | slow intro â†’ massive peak | extreme contrast | "starts quiet, massive build, explosive climax, drop" |

---

## 8. Emotion Fuser (Module E)

### Fuser System Prompt

```
You are an emotion analyst for a music generation system.

Given mood signals from one or more inputs (image, text, voice),
produce ONE unified emotional profile as JSON:

{
  "emotion": "<dominant emotion word>",
  "energy": <0-100 integer>,
  "style": <0-100 integer, 0=lo-fi intimate, 100=cinematic epic>,
  "warmth": <0-100 integer, 0=warm analog, 100=bright digital>,
  "arc": <0-100 integer, 0=steady constant, 100=big dramatic build>
}

Base your values on the emotional content of the inputs.
A sad quiet poem â†’ low energy, lo-fi style, warm, steady arc.
An action photo with excited text â†’ high energy, cinematic, bright, big build.

Return ONLY the JSON object.
```

---

## 9. Music Orchestrator (Module F)

### Responsibilities

1. **Receive** `final_profile` dict (includes slider values + emotion + overrides list)
2. **Map** each slider value to musical descriptors using the mapping table from section 7
3. **Compose** a Claude prompt that asks for a MusicGen-ready text prompt incorporating all mapped parameters
4. **Return** the prompt string

### Orchestrator System Prompt

```
You are a music director creating a prompt for an AI music generator.

Given this emotional profile:
- Emotion: {emotion}
- Energy: {energy}/100 â†’ {energy_descriptors}
- Style: {style}/100 â†’ {style_descriptors}
- Warmth: {warmth}/100 â†’ {warmth_descriptors}
- Arc: {arc}/100 â†’ {arc_descriptors}

Write a vivid 2-3 sentence music generation prompt that blends ALL
of these qualities naturally. Include specific instruments, tempo feel,
production style, and structural arc.

The listener's dominant emotion is "{emotion}" â€” the music should
honor that feeling while respecting the parameter settings above.

Output ONLY the prompt text. No labels, no JSON, no explanation.
```

### Orchestrator Internal Logic

```
1. Look up energy value in energy mapping table â†’ get descriptors
2. Look up style value in style mapping table â†’ get descriptors
3. Look up warmth value in warmth mapping table â†’ get descriptors
4. Look up arc value in arc mapping table â†’ get descriptors
5. Inject all descriptors + emotion into Claude system prompt
6. Return Claude's response as the music prompt string
```

---

## 10. Folder Structure

```
Music2myears/
â”œâ”€â”€ app.py                        # Streamlit entry point (single-screen UI)
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env                          # ANTHROPIC_API_KEY, HF_TOKEN, HF_MODEL_ID
â”œâ”€â”€ .env.example                  # Template for env vars
â”œâ”€â”€ modules/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ image_analyzer.py         # B: image â†’ mood dict
â”‚   â”œâ”€â”€ text_analyzer.py          # C: text â†’ mood dict
â”‚   â”œâ”€â”€ voice_analyzer.py         # D: voice â†’ mood dict
â”‚   â”œâ”€â”€ emotion_fuser.py          # E: mood dicts â†’ ai_profile (4 dimensions)
â”‚   â”œâ”€â”€ music_orchestrator.py     # F: final_profile â†’ music prompt
â”‚   â”œâ”€â”€ music_generator.py        # G: prompt â†’ HF API â†’ wav bytes
â”‚   â”œâ”€â”€ explainer.py              # H: profiles â†’ explanation + timeline
â”‚   â””â”€â”€ feedback.py               # I: save ratings + profiles
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ claude_client.py          # Shared Anthropic client
â”œâ”€â”€ data/
â”‚   â””â”€â”€ feedback.json             # Auto-created ratings log
â””â”€â”€ plan/
    â”œâ”€â”€ spec.md
    â”œâ”€â”€ project_plan.md
    â””â”€â”€ blueprint.md              # This file
```

---

## 11. Six-Hour Hackathon Build Plan

Module G is now an API call instead of a local model install. This is
faster to build and eliminates the GPU/memory risk entirely. The biggest
demo risk shifts to HF API cold-start latency and rate limits.

```
HOUR 0:00â€“0:30  â–¸ Setup & Skeleton
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
- Create folder structure, venv, install requirements.txt
- Create .env with ANTHROPIC_API_KEY, HF_TOKEN, HF_MODEL_ID
- Create utils/claude_client.py
- Stub every module with hardcoded returns
- Verify `streamlit run app.py` shows a blank page

HOUR 0:30â€“1:15  â–¸ MODULE G â€” Music Generator (HF API)  âš ï¸ HIGHEST RISK
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
- Implement music_generator.py:
  POST to HF Inference API with prompt, get audio bytes back
- Handle 503 cold-start (retry with backoff)
- Test with hardcoded prompt: call HF â†’ save response â†’ play wav
- Confirm audio plays in Python and in Streamlit st.audio
- WHY FIRST: if HF API is down, rate-limited, or returns
  garbage, you need to know NOW. Pivot options: different
  HF model, Replicate API, or pre-recorded fallback audio.
- NOTE: faster than v2 (no model download, no GPU debugging).
  Budget shrinks from 60min to 45min.

HOUR 1:15â€“2:15  â–¸ MODULE E+F â€” Fuser + Orchestrator (with slider schema)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
- Implement emotion_fuser.py with prompt that returns
  all 4 numeric dimensions (energy, style, warmth, arc)
- Implement music_orchestrator.py with slider mapping table
  (hardcode the 4 mapping lookups, feed to Claude prompt)
- Test end-to-end: hardcoded mood dicts â†’ ai_profile â†’
  final_profile (simulate slider override) â†’ music prompt â†’
  send to HF API â†’ confirm audio quality matches prompt intent
- WHY SECOND: bad mapping = bad music. Validate the full
  sliderâ†’promptâ†’HFâ†’audio chain before building UI.

HOUR 2:15â€“3:00  â–¸ MODULE C â€” Text Analyzer + End-to-End
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
- Implement text_analyzer.py (Claude call â†’ mood dict)
- Test full chain: text â†’ analyzer â†’ fuser â†’ orchestrator
  â†’ HF API â†’ wav
- This proves the pipeline works with real input.

HOUR 3:00â€“4:00  â–¸ MODULE A â€” Single-Screen UI with Advanced Options
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
- Text input area at top
- st.expander("Advanced Options", expanded=False) with 4 sliders
  initialized to midpoint 50 defaults
- Single [ Generate Music ] button below expander
- On click: full pipeline with spinner
  â†’ analyze â†’ fuse â†’ apply slider overrides â†’ orchestrate
  â†’ call HF API â†’ display audio
- Show ai_profile vs final_profile comparison in results
- Show music prompt in expander
- Store ai_profile in session_state for subsequent runs
- Handle HF cold-start: show "Model warming up..." message
- ğŸ¯ CHECKPOINT: working demo with text + optional sliders + audio

HOUR 4:00â€“4:45  â–¸ MODULES B+D â€” Image + Voice Analyzers
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
- Image analyzer: Claude Vision API â†’ mood dict
- Voice analyzer: Whisper transcribe â†’ Claude tone â†’ mood dict
- Add uploaders to input section of UI, wire into mood_list
- All inputs now feed the AI profile (and therefore slider defaults)

HOUR 4:45â€“5:15  â–¸ MODULE H â€” Explainer + Timeline
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
- Explainer receives both ai_profile and final_profile
- Generates explanation that references user overrides if any
- If no overrides: "The AI read your inputs and crafted..."
- Plotly bar chart for emotion/energy timeline
- Display in results section below audio player

HOUR 5:15â€“5:45  â–¸ MODULE I â€” Feedback
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
- Feedback: star rating + replay toggle â†’ save to JSON
  (saves ai_profile + final_profile + overrides)
- Submit button wired up

HOUR 5:45â€“6:00  â–¸ Polish + Safety Net
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
- Loading spinners with status messages
- Error handling for missing inputs, HF failures
- Title, branding, st.divider()
- Pre-warm HF model (one throwaway request at app start)
- Record backup demo video
```

### Demo Safety Net

| After Hour | What You Can Demo |
|-----------|------------------|
| 1:15 | HF API works (hardcoded prompt â†’ audio) |
| 2:15 | Full pipeline works (hardcoded input â†’ audio) |
| 3:00 | Real text â†’ real audio |
| **4:00** | **Full demo: text + Advanced Options sliders â†’ personalized music** |
| 4:45 | All 3 input types working |
| 5:15 | Explanation + visualization |
| 5:45 | Feedback collection |
| 6:00 | Polished, branded, backup recorded |

### Demo Story

> "Drop in a photo, some text, or a voice note. Hit Generate â€” the
> AI reads your emotions and creates a soundtrack in seconds. Want
> more control? Open Advanced Options and fine-tune the energy, style,
> warmth, and arc. Trust the AI, or shape it yourself."

### HF API Risk Mitigation

| Risk | Mitigation |
|------|-----------|
| Cold-start 503 | Retry with backoff (5s, 15s, 30s). Show "warming up" in UI. Pre-warm at app start. |
| Rate limit | Free tier: ~30 req/hr. Enough for demo. Warn if close. |
| Slow response | Set 60s timeout. Show progress spinner. |
| API down | Have 2-3 pre-generated .wav files as fallback demo audio. |
| Bad audio quality | Test prompts during Hour 0:30. If musicgen-small is poor, try musicgen-medium. |

---

## 12. Exact Prompts for Each Module

### Setup
```
Read plan/blueprint.md for full context. Then create:
1) requirements.txt with: streamlit, anthropic, requests,
   python-dotenv, Pillow, plotly, openai-whisper
2) .env.example with:
   ANTHROPIC_API_KEY=your-anthropic-key-here
   HF_TOKEN=your-huggingface-token-here
   HF_MODEL_ID=facebook/musicgen-small
3) utils/claude_client.py â€” loads .env, returns Anthropic client
4) Stub app.py showing "Music2MyEars" title in Streamlit
5) Stub all modules in modules/ with functions that return
   hardcoded dicts matching the schemas in blueprint.md
Keep it minimal. No extras.
```

### Module G â€” Music Generator (HF Inference API)
```
Read plan/blueprint.md sections on Module G (section 4)
and the environment setup (section 5).
Create modules/music_generator.py.
- Load HF_TOKEN and HF_MODEL_ID from environment (.env)
- Function: generate_music(prompt: str, duration_sec=10)
  â†’ POST to https://api-inference.huggingface.co/models/{HF_MODEL_ID}
  â†’ Headers: {"Authorization": "Bearer {HF_TOKEN}"}
  â†’ Body: {"inputs": prompt}
  â†’ Returns list of 2 audio byte buffers:
    - First call with original prompt
    - Second call with prompt + " with subtle variation"
- Handle 503 cold-start: retry up to 3 times with backoff
  (5s, 15s, 30s). Raise clear error if still failing.
- Handle 401: raise with "Check your HF_TOKEN" message.
- Include __main__ block that tests with a hardcoded prompt,
  saves result to test_output.wav, prints success/failure.
Just functions, no classes. Use requests library.
```

### Module E â€” Emotion Fuser
```
Read plan/blueprint.md sections on Module E (section 8)
and the emotional profile schema (section 2).
Create modules/emotion_fuser.py.
Function: fuse_emotions(mood_list: list[dict]) -> dict
- Calls Claude with the fuser system prompt from the blueprint
- Must return ai_profile with ALL fields: emotion (str),
  energy (int 0-100), style (int 0-100), warmth (int 0-100),
  arc (int 0-100), sources (list)
- Parse JSON response, validate all fields present
- Use utils/claude_client.py
```

### Module F â€” Music Orchestrator
```
Read plan/blueprint.md sections on Module F (section 9)
and the slider mapping table (section 7).
Create modules/music_orchestrator.py.
Function: create_music_prompt(final_profile: dict) -> str
- Implement the 4 mapping lookups from blueprint section 7:
  energyâ†’tempo/dynamics, styleâ†’instruments/production,
  warmthâ†’tone/textures, arcâ†’structure/dynamics
  (use simple if/elif ranges, no over-engineering)
- Inject mapped descriptors into the orchestrator system
  prompt from the blueprint
- Call Claude, return raw prompt string
- Use utils/claude_client.py
```

### Module C â€” Text Analyzer
```
Read plan/blueprint.md section on Module C.
Create modules/text_analyzer.py.
Function: analyze_text(text: str) -> dict
- Claude call: analyze text, return JSON with summary,
  mood, energy (0.0-1.0)
- Parse response, add source: "text", return dict
- Use utils/claude_client.py
```

### Module A â€” Single-Screen UI with Advanced Options
```
Read plan/blueprint.md sections on Module A (section 6),
the single-screen layout, interaction flow, and slider
override logic.
Update app.py to build a SINGLE-SCREEN Streamlit app:

TOP OF PAGE:
- Title "Music2MyEars" with subtitle
- Text input area (later: image + voice uploaders)

MIDDLE:
- st.expander("Advanced Options", expanded=False) containing:
  - 4 sliders: Energy (0-100, default 50), Style (0-100, default 50),
    Warmth (0-100, default 50), Arc (0-100, default 50)
  - Labels: "Lo-fi â†” Cinematic", "Warm â†” Bright", "Steady â†” Big Build"
  - Small note: "Defaults will be set by AI after analysis"

BELOW EXPANDER:
- [ Generate Music ] button

ON CLICK (one spinner, full pipeline):
  1. Run text_analyzer â†’ emotion_fuser â†’ get ai_profile
  2. Read slider values. If slider != 50, treat as override.
     Build final_profile with overrides list.
     If no overrides, final_profile = ai_profile.
  3. Run music_orchestrator(final_profile) â†’ music_generator
     (music_generator now calls HF API, not local model)
  4. Show results: detected emotion, AI vs Final profile,
     audio players, music prompt in expander
  5. Store ai_profile in session_state. On next run, sliders
     default to previous ai_profile values instead of 50.

Wire real modules. Must work end-to-end with text input.
```

### Module B â€” Image Analyzer
```
Read plan/blueprint.md section on Module B.
Create modules/image_analyzer.py.
Function: analyze_image(image_bytes: bytes) -> dict
- Send image to Claude Vision API
- Prompt: describe image, return JSON with caption, mood,
  energy (0.0-1.0)
- Parse, add source: "image", return dict
Then update app.py: add image uploader in the input section
(above the Advanced Options expander). If image provided,
include its analysis in mood_list passed to fuser.
```

### Module D â€” Voice Analyzer
```
Read plan/blueprint.md section on Module D.
Create modules/voice_analyzer.py.
Function: analyze_voice(audio_bytes: bytes) -> dict
- Whisper (model="base") to transcribe (still runs locally)
- Claude call on transcript for tone/mood â†’ JSON with
  transcript, mood, energy (0.0-1.0)
- Add source: "voice", return dict
Then update app.py: add audio file uploader (wav/mp3) in the
input section alongside image and text. Wire into mood_list.
```

### Module H â€” Explainer + Timeline
```
Read plan/blueprint.md section on Module H.
Create modules/explainer.py.
Function: explain_music(ai_profile, final_profile, music_prompt) -> dict
- Takes BOTH profiles to reference overrides
- Claude call: "Given the AI-detected profile and the user's
  final settings (overrides: {overrides}), write 2-3 warm
  sentences explaining why this music was created. If the user
  overrode values, mention how their adjustments shaped the
  result. If no overrides, explain how the AI interpreted
  their input."
- Also return timeline: list of {section, emotion, energy}
Then update app.py: show explanation in st.info callout,
show Plotly bar chart of timeline in results section.
```

### Module I â€” Feedback + Polish
```
Read plan/blueprint.md section on Module I.
Create modules/feedback.py.
Function: save_feedback(session_id, rating, would_replay,
                        ai_profile, final_profile)
- Append to data/feedback.json (create if missing)
- Include timestamp, all profile data, overrides list
Then update app.py:
- Star rating slider (1-5) in results section
- "Would you listen again?" toggle
- Submit button â†’ save_feedback
- Add st.divider() between sections
- Loading spinner with status messages during generation
  (include "Model warming up..." for HF cold-start)
- Error: require at least one input before Generate
```

---

## Appendix: Key Design Decisions

1. **Why single-screen with collapsible Advanced Options?** Zero friction for the default path â€” user uploads, clicks Generate, hears music. Power users expand the panel. No phase gates, no extra clicks, no waiting for intermediate results. One screen, one button, one experience.

2. **Why HF Inference API instead of local MusicGen?** No GPU required. No 2GB model download. No torch version hell. Setup drops from 30+ minutes to `pip install requests`. The trade-off is cold-start latency (first request takes ~30s while HF loads the model) and rate limits (~30 req/hr free tier). Both are acceptable for a hackathon demo.

3. **Why integer 0â€“100 instead of float 0â€“1?** Sliders with integer steps feel more tactile and readable. "Energy: 72" is more intuitive than "Energy: 0.72" in a demo.

4. **Why track overrides?** The explainer creates a much more compelling narrative when it can say "you chose to..." vs just describing the final values. It makes the user feel heard.

5. **Why map sliders to words, not numbers?** MusicGen is a text-conditioned model. It responds to descriptive language ("soaring orchestral strings") not parameter values ("style=85"). The mapping table bridges that gap.

6. **Why midpoint defaults before first analysis?** The sliders must exist in the expander before the pipeline runs. Starting at 50 (neutral) means untouched sliders are detectable. After the first run, session_state holds the real ai_profile for smarter defaults on re-generation.

7. **Why keep Whisper local but not MusicGen?** Whisper-base is tiny (~150MB), runs fast on CPU, and has no API alternative that's simpler. MusicGen is large, GPU-hungry, and HF Inference API handles it perfectly. Different trade-offs, different choices.

8. **Demo narrative**: "Users can trust the AI â€” or fine-tune with Advanced Options." The collapsed expander communicates: this is optional. The sliders communicate: you have power.
